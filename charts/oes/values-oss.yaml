#####################################################
## Spinnaker configuration
#####################################################
# Default values for Spinnaker chart.
## This is a YAML-formatted file.
## Declare variables to be passed into your templates.

## Option to select spinnaker as required helm chart
## Setting this to true, will ensure Open Source Spinnaker is installed
installSpinnaker: true

## Installation mode
## Available installation mode - None
## Do not change this field; OES installation shall be skipped
installationMode: None

## Set to true to expose spinnaker and deck services as LoadBalancers
##
createIngress: false

## OES UI & Gate service type
##
k8sServiceType: LoadBalancer

#####################################################
# Spinnaker instance configuration
#####################################################
spinnaker:
  halyard:
    spinnakerVersion: 1.24.4
    image:
      repository: quay.io/opsmxpublic/ubi8-spin-halyard
      tag: opsmx-1.40.0
    # Set to false to disable persistence data volume for halyard
    persistence:
      enabled: true
    # Provide a config map with Hal commands that will be run the core config (storage)
    # The config map should contain a script in the config.sh key
    additionalScripts:
      enabled: false
      configMapName: my-halyard-config
      configMapKey: config.sh
      # If you'd rather do an inline script, set create to true and put the content in the data dict like you would a configmap
      # The content will be passed through `tpl`, so value interpolation is supported.
      create: false
      data: {}
    additionalSecrets:
      create: false
      data: {}
      ## Uncomment if you want to use a pre-created secret rather than feeding data in via helm.
      # name:
    additionalConfigMaps:
      create: false
      data: {}
      ## Uncomment if you want to use a pre-created ConfigMap rather than feeding data in via helm.
      # name:
    additionalProfileConfigMaps:
      data:
        ## if you're running spinnaker behind a reverse proxy such as a GCE ingress
        ## you may need the following profile settings for the gate profile.
        ## see https://github.com/spinnaker/spinnaker/issues/1630
        ## otherwise its harmless and will likely become default behavior in the future
        ## According to the linked github issue.
        gate-local.yml:
          server:
            tomcat:
              protocolHeader: X-Forwarded-Proto
              remoteIpHeader: X-Forwarded-For
              internalProxies: .*
              httpsServerPort: X-Forwarded-Port

    ## Define custom settings for Spinnaker services. Read more for details:
    ## https://www.spinnaker.io/reference/halyard/custom/#custom-service-settings
    ## You can use it to add annotations for pods, override the image, etc.
    additionalServiceSettings:
    ## artifactId to override Spinnaker components images with OpsMx custom images
      gate.yml:
        healthEndpoint: /health
        kubernetes:
          useExecHealthCheck: false
        artifactId: quay.io/opsmxpublic/ubi8-spin-gate:1.20.0
      deck.yml:
        artifactId: quay.io/opsmxpublic/ubi8-spin-deck:3.5.1
      clouddriver.yml:
        artifactId: quay.io/opsmxpublic/ubi8-spin-clouddriver:7.2.2
      echo.yml:
        artifactId: quay.io/opsmxpublic/ubi8-spin-echo:2.15.2
      fiat.yml:
        artifactId: quay.io/opsmxpublic/ubi8-spin-fiat:1.14.1
      front50.yml:
        artifactId: quay.io/opsmxpublic/ubi8-spin-front50:0.26.1
      igor.yml:
        artifactId: quay.io/opsmxpublic/ubi8-spin-igor:1.14.0
      kayenta.yml:
        artifactId: quay.io/opsmxpublic/ubi8-spin-kayenta:0.19.0
      orca.yml:
        artifactId: quay.io/opsmxpublic/ubi8-spin-orca:2.18.1
      rosco.yml:
        artifactId: quay.io/opsmxpublic/ubi8-spin-rosco:0.23.0
    ## Uncomment if you want to add extra commands to the init script
    ## run by the init container before halyard is started.
    ## The content will be passed through `tpl`, so value interpolation is supported.
    # additionalInitScript: |-

    ## Uncomment if you want to add annotations on halyard and install-using-hal pods
    # annotations:
    #   iam.amazonaws.com/role: <role_arn>

    ## Uncomment the following resources definitions to control the cpu and memory
    # resources allocated for the halyard pod
    resources: {}
      # requests:
      #   memory: "1Gi"
      #   cpu: "100m"
      # limits:
      #   memory: "2Gi"
      #   cpu: "200m"

    ## Uncomment if you want to set environment variables on the Halyard pod.
    # env:
    #   - name: JAVA_OPTS
    #     value: -Dhttp.proxyHost=proxy.example.com
    customCerts:
      ## Enable to override the default cacerts with your own one
      enabled: false
      secretName: custom-cacerts

  # Define which registries and repositories you want available in your
  # Spinnaker pipeline definitions
  # For more info visit:
  #   https://www.spinnaker.io/setup/providers/docker-registry/

  # Configure your Docker registries here
  dockerRegistries:
  - name: dockerhub
    address: index.docker.io
    repositories:
      - library/alpine
      - library/ubuntu
      - library/centos
      - library/nginx
  # - name: gcr
  #   address: https://gcr.io
  #   username: _json_key
  #   password: '<INSERT YOUR SERVICE ACCOUNT JSON HERE>'
  #   email: 1234@5678.com

  # If you don't want to put your passwords into a values file
  # you can use a pre-created secret instead of putting passwords
  # (specify secret name in below `dockerRegistryAccountSecret`)
  # per account above with data in the format:
  # <name>: <password>

  # dockerRegistryAccountSecret: myregistry-secrets

  kubeConfig:
    # Use this when you want to register arbitrary clusters with Spinnaker
    # Upload your ~/kube/.config to a secret
    enabled: false
    secretName: my-kubeconfig
    secretKey: config
    # List of contexts from the kubeconfig to make available to Spinnaker
    contexts:
    - default
    deploymentContext: default
    omittedNameSpaces:
    - kube-system
    - kube-public
    onlySpinnakerManaged:
      enabled: true

  # Change this if youd like to expose Spinnaker outside the cluster
  ingress:
    enabled: false
    # host: spinnaker.example.org
    # annotations:
      # ingress.kubernetes.io/ssl-redirect: 'true'
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    # tls:
    #  - secretName: -tls
    #    hosts:
    #      - domain.com

  ingressGate:
    enabled: false
    # host: gate.spinnaker.example.org
    # annotations:
      # ingress.kubernetes.io/ssl-redirect: 'true'
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    # tls:
    #  - secretName: -tls
    #    hosts:
    #      - domain.com

  # spinnakerFeatureFlags is a list of Spinnaker feature flags to enable
  # Ref: https://www.spinnaker.io/reference/halyard/commands/#hal-config-features-edit
  # spinnakerFeatureFlags:
  #   - artifacts
  #   - pipeline-templates
  spinnakerFeatureFlags:
    - pipeline-templates
    - managed-pipeline-templates-v2-ui

  # Node labels for pod assignment
  # Ref: https://kubernetes.io/docs/user-guide/node-selection/
  # nodeSelector to provide to each of the Spinnaker components
  nodeSelector: {}

  # Redis password to use for the in-cluster redis service
  # Enable redis to use in-cluster redis
  redis:
    enabled: true
    # External Redis option will be enabled if in-cluster redis is disabled
    external:
      host: "<EXTERNAL-REDIS-HOST-NAME>"
      port: 6379
      # password: ""
    password: password
    nodeSelector: {}
    cluster:
      enabled: false
  # Uncomment if you don't want to create a PVC for redis
  #  master:
  #    persistence:
  #      enabled: false

  # Minio access/secret keys for the in-cluster S3 usage
  # Minio is not exposed publically
  minio:
    enabled: true
    imageTag: RELEASE.2019-02-13T19-48-27Z
    serviceType: ClusterIP
    accessKey: spinnakeradmin
    secretKey: spinnakeradmin
    bucket: "spinnaker"
    nodeSelector: {}
    persistence:
      enabled: true
      size: 10Gi

  # Google Cloud Storage
  gcs:
    enabled: false
    project: my-project-name
    bucket: "<GCS-BUCKET-NAME>"
    ## if jsonKey is set, will create a secret containing it
    jsonKey: '<INSERT CLOUD STORAGE JSON HERE>'
    ## override the name of the secret to use for jsonKey, if `jsonKey`
    ## is empty, it will not create a secret assuming you are creating one
    ## external to the chart. the key for that secret should be `key.json`.
    secretName:

  # AWS Simple Storage Service
  s3:
    enabled: false
    bucket: "<S3-BUCKET-NAME>"
    # rootFolder: "front50"
    # region: "us-east-1"
    # endpoint: ""
    # accessKey: ""
    # secretKey: ""

  # Azure Storage Account
  azs:
    enabled: false
  #   storageAccountName: ""
  #   accessKey: ""
  #   containerName: "spinnaker"

  rbac:
    # Specifies whether RBAC resources should be created
    create: true

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccounts to use.
    # If left blank it is auto-generated from the fullname of the release
    halyardName:
    spinnakerName:
  securityContext:
    # Specifies permissions to write for user/group
    runAsUser: 1000
    fsGroup: 1000

  ## Option to enable HA in Spinnaker; Enabled by default
  enableHA: true

  ## Centralized Monitoring enable for Prometheus
  enableCentralMonitoring: false

  # Initialize gitOps style Halyard
  gitopsHalyard:
    enabled: false
    mTLS:
      enabled: false   # Enable mTLS for Spinnaker Services and SSL for Deck and Gate
      deckIngressHost: spindeck.engineering.opsmx.net  # Used by OES to create ingress for deck
      gateIngressHost: spingate.engineering.opsmx.net  # Used by OES to create ingress for gate
    repo-type: git # git, S3, vault, only git support at this point
    secretName: opsmx-gitops-auth
    # Max time(in secs) that an init container of halyard should wait
    # to fetch External Load Balancer IP of spin-deck and spin-gate
    spinnakerLBCheckDelay: 180
    # Promote applications and pipelines from one environment to another environment
      ## x509 authentication for Spinnaker Gate
    gatex509:
      enabled: false
      host: spingate-x509.engineering.opsmx.net
    pipelinePromotion:
      enabled: false
      ## Namespace in which pipeline promotion job need to run
      # TODO: Implementation pending; currently default namespace is hardcoded
      executionNamespace: default
      ## Spinnaker kubernetes account to use for job execution
      # TODO: Implementation pending; currently default namespace is hardcoded
      executionAccountName: default

####################################################
## LDAP configuration
####################################################
## Set it to false if own LDAP is to be configured
##
installOpenLdap: true
## OpenLDAP custom configuration; will override default configuration of
## openldap helm chart
##
openldap:
  # Password for the admin user; by default it is set to admin
  adminPassword: opsmxadmin123
  configPassword: opsmxconfig123
  omitClusterIP: true
  persistence:
    enabled: true
  env:
    LDAP_REMOVE_CONFIG_AFTER_SETUP: "false"

  customLdifFiles:
    01-memberof.ldif: |-
      dn: cn=module,cn=config
      cn: module
      objectClass: olcModuleList
      olcModuleLoad: memberof.la
      olcModulePath: /usr/lib/ldap

      dn: olcOverlay={0}memberof,olcDatabase={1}hdb,cn=config
      objectClass: olcConfig
      objectClass: olcMemberOf
      objectClass: olcOverlayConfig
      objectClass: top
      olcOverlay: memberof
      olcMemberOfDangling: ignore
      olcMemberOfRefInt: TRUE
      olcMemberOfGroupOC: groupOfNames
      olcMemberOfMemberAD: member
      olcMemberOfMemberOfAD: memberOf
    02-refint1.ldif: |-
      dn: cn=module{1},cn=config
      changetype: modify
      add: olcmoduleload
      olcmoduleload: refint.la
    03-refint2.ldif: |-
      dn: olcOverlay={1}refint,olcDatabase={1}hdb,cn=config
      objectClass: olcConfig
      objectClass: olcOverlayConfig
      objectClass: olcRefintConfig
      objectClass: top
      olcOverlay: {1}refint
      olcRefintAttribute: memberof member manager owner
    04-add_ou.ldif: |-
      dn: ou=groups,dc=example,dc=org
      objectClass: organizationalUnit
      ou: Groups
    05-admin.ldif: |-
      dn: cn=admin,ou=groups,dc=example,dc=org
      objectClass: groupofnames
      cn: admin
      description: read write and execute group
      member: cn=admin,dc=example,dc=org
    06-developer.ldif: |-
      dn: cn=developers,ou=groups,dc=example,dc=org
      objectClass: groupofnames
      cn: developers
      description: read only users
      member: cn=admin,dc=example,dc=org
      member: cn=developer,dc=example,dc=org
    07-qa.ldif: |-
      dn: cn=QA,ou=groups,dc=example,dc=org
      objectClass: groupofnames
      cn: QA
      description: read only users
      member: cn=admin,dc=example,dc=org
      member: cn=qa,dc=example,dc=org
    08-manager.ldif: |-
      dn: cn=managers,ou=groups,dc=example,dc=org
      objectClass: groupofnames
      cn: managers
      description: read and execute group
      member: cn=admin,dc=example,dc=org
      member: cn=manager,dc=example,dc=org
    09-IT-manager.ldif: |-
      dn: cn=ITManagers,ou=groups,dc=example,dc=org
      objectClass: groupofnames
      cn: ITManagers
      description: read and execute group
      member: cn=admin,dc=example,dc=org
      member: cn=ITManager,dc=example,dc=org

## ldap configuration used in oes-gate & oes-platform for authentication
## and authorization
ldap:
  enabled: true
  url: ldap://{{ .Release.Name }}-openldap:389
  userDnPattern: cn={0}
  basedn: dc=example,dc=org # Value of ldap.base.dn
  adminDn: cn=admin,dc=example,dc=org
  adminPassword: opsmxadmin123
  userDisplayName: displayName
  pattern: (&(cn=USERNAME))
  userGroupIdentity: memberOf
  userIdentity: cn
  userPrepend: cn=USERNAME

#####################################################
# Centralized Logging Configuration
#####################################################
enableCentralLogging: false
elasticsearch:
  replicas: 1
  minimumMasterNodes: 1
  resources:
    requests:
      cpu: "100m"
      memory: "1Gi"

kibana:
  service:
    type: LoadBalancer
  resources:
    requests:
      cpu: "100m"
      memory: "250Mi"
  lifecycle:
    postStart:
      exec:
        command:
          - bash
          - -c
          - >
            until curl localhost:5601; do echo "Waiting for Kibana to be available..."; sleep 5; done;
            until curl elasticsearch-master:9200; do echo "Waiting for Elasticsearch to be available..."; sleep 5; done;
            sleep 60;
            curl https://raw.githubusercontent.com/OpsMx/enterprise-spinnaker/master/scripts/kibana/kibana_objects.ndjson > /tmp/kibana_objects.ndjson;
            curl -X POST "localhost:5601/api/saved_objects/_import?overwrite=true" -H "kbn-xsrf: true" --form file=@/tmp/kibana_objects.ndjson 2>&1 1> /tmp/postStart.out;
