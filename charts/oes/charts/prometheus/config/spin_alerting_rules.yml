groups:
- name: spinnaker-service-is-down
  rules:
  - alert: clouddriver-rw-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Clouddriver-rw Spinnaker service is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-clouddriver-rw"} == 0
    labels:
      severity: critical
  - alert: clouddriver-ro-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Clouddriver-ro Spinnaker service is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-clouddriver-ro"} == 0
    labels:
      severity: critical
  - alert: clouddriver-caching-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Clouddriver-caching Spinnaker service is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-clouddriver-caching"} == 0
    labels:
      severity: critical
  - alert: clouddriver-ro-deck-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Clouddriver-ro-deck Spinnaker service is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-clouddriver-ro-deck"} == 0
    labels:
      severity: critical
  - alert: gate-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Gate Spinnaker services is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-gate"} == 0
    labels:
      severity: critical
  - alert: orca-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Orca Spinnaker service is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-orca"} == 0
    labels:
      severity: critical
  - alert: igor-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Igor Spinnaker service is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-igor"} == 0
    labels:
      severity: critical
  - alert: echo-scheduler-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Echo-Scheduler Spinnaker service is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-echo-scheduler"} == 0
    labels:
      severity: critical
  - alert: echo-worker-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Echo-worker Spinnaker service is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-echo-worker"} == 0
    labels:
      severity: critical
  - alert: front50-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Front50 Spinnaker service is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-front50"} == 0
    labels:
      severity: critical
  - alert: fiat-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Fiat Spinnaker service is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-fiat"} == 0
    labels:
      severity: critical
  - alert: rosco-is-down
    annotations:
      description: 'Service {{$labels.service}} with pod name {{$labels.pod}} in namespace {{$labels.namespace}}
        is not responding'
      summary: Rosco Spinnaker service is down
    expr: up{job="opsmx_spinnaker_metrics", service="spin-rosco"} == 0
    labels:
      severity: critical
- name: latency-too-high
  rules:
  - alert: clouddriver-ro-latency-too-high
    expr:  sum(rate(clouddriver_ro:controller:invocations__total{service="spin-clouddriver-ro"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(clouddriver_ro:controller:invocations__count_total{service="spin-clouddriver-ro"}[5m])) by (controller, instance, method, success, statusCode, service, namespace) > 1
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is {{$value}} seconds for {{ $labels }}"
      summary: Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
  - alert: clouddriver-rw-latency-too-high
    expr: sum(rate(clouddriver_rw:controller:invocations__total{service="spin-clouddriver-rw"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(clouddriver_rw:controller:invocations__count_total{service="spin-clouddriver-rw"}[5m])) by (controller, instance, method, success, statusCode, service, namespace) > 0.5
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is ({{$value}} seconds for {{ $labels }}"
      summary:  Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
  - alert: clouddriver-caching-latency-too-high
    expr: sum(rate(clouddriver_caching:controller:invocations__total{service="spin-clouddriver-caching"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(clouddriver_caching:controller:invocations__count_total{service="spin-clouddriver-caching"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)  > 5
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is {{$value}} seconds for {{ $labels }}"
      summary: Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
  - alert: clouddriver_ro_deck-latency-too-high
    expr:  sum(rate(clouddriver_ro_deck:controller:invocations__total{service="spin-clouddriver-ro-deck"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(clouddriver_ro_deck:controller:invocations__total{service="spin-clouddriver-ro-deck"}[5m])) by (controller, instance, method, success, statusCode, service, namespace) > 5
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is {{$value}} seconds for {{ $labels }}"
      summary: Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
  - alert: gate-latency-too-high
    expr: sum(rate(gate:controller:invocations__total{service="spin-gate"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(gate:controller:invocations__count_total{service="spin-gate"}[5m])) by (controller, instance, method, success, statusCode, service, namespace) > 0.5
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is {{$value}} seconds for {{ $labels }}"
      summary: Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
  - alert: orca-latency-too-high
    expr: sum(rate(orca:controller:invocations__total{service="spin-orca"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(orca:controller:invocations__count_total{service="spin-orca"}[5m])) by (controller, instance, method, success, statusCode, service, namespace) > 0.5
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is {{$value}} seconds for {{ $labels }}"
      summary: Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
  - alert: igor-latency-too-high
    expr: sum(rate(igor:controller:invocations__total{service="spin-igor"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(igor:controller:invocations__count_total{service="spin-igor"}[5m])) by (controller, instance, method, success, statusCode, service, namespace) > 0.5
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is {{$value}} seconds for {{ $labels }}"
      summary: Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
  - alert: echo_scheduler-latency-too-high
    expr: sum(rate(echo_scheduler:controller:invocations__total{service="spin-echo-scheduler"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(echo_scheduler:controller:invocations__count_total{service="spin-echo-scheduler"}[5m])) by (controller, instance, method, success, statusCode, service, namespace) > 0.5
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is {{$value}} seconds for {{ $labels }}"
      summary: Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
  - alert: echo_worker-latency-too-high
    expr:  sum(rate(echo_worker:controller:invocations__total{service="spin-echo-worker"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(echo_worker:controller:invocations__count_total{service="spin-echo-worker"}[5m])) by (controller, instance, method, success, statusCode, service, namespace) > 0.5
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is {{$value}} seconds for {{ $labels }}"
      summary: Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
  - alert: front50-latency-too-high
    expr: sum(rate(front50:controller:invocations__total{service="spin-front50"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(front50:controller:invocations__count_total{service="spin-front50"}[5m])) by (controller, instance, method, success, statusCode, service, namespace) > 0.5
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is {{$value}} seconds for {{ $labels }}"
      summary: Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
  - alert: fiat-latency-too-high
    expr: sum(rate(fiat:controller:invocations__total{service="spin-fiat"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(fiat:controller:invocations__count_total{service="spin-fiat"}[5m])) by (controller, instance, method, success, statusCode, service, namespace) > 0.5
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is {{$value}} seconds for {{ $labels }}"
      summary: Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
  - alert: rosco-latency-too-high
    expr: sum(rate(rosco:controller:invocations__total{service="spin-rosco"}[5m])) by (controller, instance, method, success, statusCode, service, namespace)/ sum(rate(rosco:controller:invocations__count_total{service="spin-rosco"}[5m])) by (controller, instance, method, success, statusCode, service, namespace) > 0.5
    for: 15m
    labels:
      severity: warning
    annotations:
      description: "Latency of the Service {{$labels.service}} is {{$value}} seconds for {{ $labels }}"
      summary: Latency of the service {{ $labels.service }} in namespace {{$labels.namespace}} is high
- name: jvm-too-high
  rules:
  - alert: clouddriver-rw-pod-may-be-evicted-soon
    annotations:
      description: Service {{$labels.service}} in namespace {{$labels.namespace}}
        may be evicted soon
      summary: Clouddriver-rw JVM memory too high
    expr: (sum(clouddriver_rw:jvm:memory:used__value) by (instance, area) / sum(clouddriver_rw:jvm:memory:max__value) by (instance, area)) > .9
    labels:
      severity: warning
  - alert: clouddriver-ro-pod-may-be-evicted-soon
    annotations:
      description: Service {{$labels.service}} in namespace {{$labels.namespace}}
        may be evicted soon
      summary: Clouddriver-ro JVM memory too high
    expr: (sum(clouddriver_ro:jvm:memory:used__value) by (instance, area) / sum(clouddriver_ro:jvm:memory:max__value) by (instance, area)) > .9
    labels:
      severity: warning
  - alert: clouddriver-caching-pod-may-be-evicted-soon
    annotations:
      description: Service {{$labels.service}} in namespace {{$labels.namespace}}
        may be evicted soon
      summary: Clouddriver-caching JVM memory too high
    expr: (sum(clouddriver_caching:jvm:memory:used__value) by (instance, area) / sum(clouddriver_caching:jvm:memory:max__value) by (instance, area)) > .9
    labels:
      severity: warning
  - alert: gate-pod-may-be-evicted-soon
    annotations:
      description: Service {{$labels.service}} in namespace {{$labels.namespace}} may be evicted soon
      summary: gate JVM memory too high
    expr: (sum(gate:jvm:memory:used__value) by (instance, area) / sum(gate:jvm:memory:max__value) by (instance, area)) > .9
    labels:
      severity: warning
  - alert: orca-pod-may-be-evicted-soon
    annotations:
      description: Service {{$labels.service}} in namespace {{$labels.namespace}}
        may be evicted soon
      summary: orca JVM memory too high
    expr: (sum(orca:jvm:gc:liveDataSize__value) by (instance, area) / sum(orca:jvm:gc:maxDataSize__value) by (instance, area)) > .9
    labels:
      severity: warning
  - alert: igor-pod-may-be-evicted-soon
    annotations:
      description: Service {{$labels.service}} in namespace {{$labels.namespace}}
        may be evicted soon
      summary: igor JVM memory too high
    expr: (sum(igor:jvm:memory:used__value) by (instance, area) / sum(igor:jvm:memory:max__value) by (instance, area)) > .9
    labels:
      severity: warning
  - alert: echo-scheduler-pod-may-be-evicted-soon
    annotations:
      description: Service {{$labels.service}} in namespace {{$labels.namespace}}
        may be evicted soon
      summary: echo-scheduler JVM memory too high
    expr: (sum(echo_scheduler:jvm:memory:used__value) by (instance, area) / sum(echo_scheduler:jvm:memory:max__value) by (instance, area)) > .9
    labels:
      severity: warning
  - alert: echo-worker-pod-may-be-evicted-soon
    annotations:
      description: Service {{$labels.service}} in namespace {{$labels.namespace}}
        may be evicted soon
      summary: echo-worker JVM memory too high
    expr: (sum(echo_worker:jvm:memory:used__value) by (instance, area) / sum(echo_worker:jvm:memory:max__value) by (instance, area)) > .9
    labels:
      severity: warning
  - alert: front50-pod-may-be-evicted-soon
    annotations:
      description: Service {{$labels.service}} in namespace {{$labels.namespace}}
        may be evicted soon
      summary: Front50 JVM memory too high
    expr: (sum(front50:jvm:memory:used__value) by (instance, area) / sum(front50:jvm:memory:max__value) by (instance, area)) > .9
    labels:
      severity: warning
- name: Front50-cache
  rules:
  - alert: "front50:storageServiceSupport:cacheAge__value"
    annotations:
      description: front50 cacheAge for {{$labels.pod}} in namespace {{$labels.namespace}} has value = {{$value}}
      summary: "front50 cacheAge too high"
    expr: front50:storageServiceSupport:cacheAge__value > 300000
    for: 2m
    labels:
      severity: warning
- name: orca-queue-issue
  rules:
  - alert: orca-queue-depth-high
    annotations:
      description: Service {{$labels.service}} in namespace {{$labels.namespace}}
        may be evicted soon
      summary: Orca queue depth is high
    expr: (sum(orca:queue:ready:depth__value{namespace!=""}) by (instance)) > 10
    labels:
      severity: warning
  - alert: orca-queue-lag-high
    annotations:
      description: Service {{$labels.service}} in namespace {{$labels.namespace}} has Lag value of {{$value}}
      summary: Orca queue lag is high
    expr: sum(rate (orca:controller:invocations__total[2m])) by (instance, service, namespace)  / sum(rate(orca:controller:invocations__count_total[2m])) by (instance, service, namespace) > 0.5
    labels:
      severity: warning
- name: igor-needs-attention
  rules:
  - alert: igor-needs-attention
    annotations:
      description: Igor in namespace {{$labels.namespace}} needs human help
      summary: Igor needs attention
    expr: igor:pollingMonitor:itemsOverThreshold__value > 0
    labels:
      severity: crtical
- name: autopilot-scrape-target-is-down
  rules:
  - alert: oes-audit-client-scrape-target-is-down
    annotations:
      description: The scrape target endpoint of component {{$labels.component}} in namespace {{$labels.kubernetes_namespace}} is down
      summary: oes-audit-client scrape target is down
    expr: up{component="auditclient"}==0
    labels:
      severity: critical
  - alert: oes-autopilot-scrape-target-is-down
    annotations:
      description: The scrape target endpoint of component {{$labels.component}} in namespace {{$labels.kubernetes_namespace}} is down
      summary: oes-autopilot scrape target is down
    expr: up{component="autopilot"}==0
    labels:
      severity: critical
  - alert: oes-dashboard-scrape-target-is-down
    annotations:
      description:  The scrape target endpoint of component {{$labels.component}} in namespace {{$labels.kubernetes_namespace}} is down
      summary: oes-dashboard scrape target is down
    expr: up{component="dashboard"}==0
    labels:
      severity: critical
  - alert: oes-platform-scrape-target-is-down
    annotations:
      description: The scrape target endpoint of component {{$labels.component}} in namespace {{$labels.kubernetes_namespace}} is down
      summary: oes-platform scrape target is down
    expr: up{component="platform"}==0
    labels:
      severity: critical
  - alert: oes-sapor-scrape-target-is-down
    annotations:
      description: The scrape target endpoint of component {{$labels.component}} in namespace {{$labels.kubernetes_namespace}} is down
      summary: oes-sapor scrape target is down
    expr: up{component="sapor"}==0
    labels:
      severity: critical
  - alert: oes-visibility-scrape-target-is-down
    annotations:
      description: The scrape target endpoint of component {{$labels.component}} in namespace {{$labels.kubernetes_namespace}} is down
      summary: oes-visibility scrape target is down
    expr: up{component="visibility"}==0
    labels:
      severity: critical
- name: prometheus-job-down
  rules:
  - alert: prometheus-job-is-down
    expr: up{job="prometheus"} == 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: The Default Prometheus Job is Down (job {{ $labels.job}})
      description: "Default Prometheus Job is Down LABELS = {{ $labels }}"
- name: volume-is-almost-full (< 10% left)
  rules:
  - alert: pvc-storage-full
    expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: Kubernetes Volume running out of disk space for (persistentvolumeclaim {{ $labels.persistentvolumeclaim }} in namespace {{$labels.namespace}})
      description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
- name: kubernetes-api-server-experiencing-high-error-rate
  rules:
  - alert: kube-api-server-errors
    expr: sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"^(?:5..)$"}[2m])) / sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[2m])) * 100 > 3
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: Kubernetes API server errors (instance {{ $labels.instance }})
      description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
- name: autopilot-component-latency-too-high
  rules:
  - alert: oes-audit-client-latency-too-high
    expr: sum(rate(http_server_requests_seconds_sum{component="auditclient"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri)/sum(rate(http_server_requests_seconds_count{component="auditclient"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri) > 0.5
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: Latency of the component {{ $labels.component }} in namespace {{$labels.kubernetes_namespace}} is high
      description: "Latency of the component {{ $labels.component }} is {{ $value }} seconds for {{ $labels }}"
  - alert: oes-autopilot-latency-too-high
    expr: sum(rate(http_server_requests_seconds_sum{component="autopilot"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri)/sum(rate(http_server_requests_seconds_count{component="autopilot"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri) > 0.5
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: Latency of the component {{ $labels.component }} in namespace {{$labels.kubernetes_namespace}} is high
      description: "Latency of the component {{ $labels.component }} is {{ $value }} seconds for {{ $labels }}"
  - alert: oes-dashboard-latency-too-high
    expr: sum(rate(http_server_requests_seconds_sum{component="dashboard"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri)/sum(rate(http_server_requests_seconds_count{component="dashboard"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri) > 0.5
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: Latency of the component {{ $labels.component }} in namespace {{$labels.kubernetes_namespace}} is high
      description: "Latency of the component {{ $labels.component }} is {{ $value }} seconds for {{ $labels }}"
  - alert: oes-platform-latency-too-high
    expr: sum(rate(http_server_requests_seconds_sum{component="platform"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri)/sum(rate(http_server_requests_seconds_count{component="platform"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri) > 0.5
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: Latency of the component {{ $labels.component }} in namespace {{$labels.kubernetes_namespace}} is high
      description: "Latency of the component {{ $labels.component }} is {{ $value }} seconds for {{ $labels }}"
  - alert: oes-sapor-latency-too-high
    expr: sum(rate(http_server_requests_seconds_sum{component="sapor"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri)/sum(rate(http_server_requests_seconds_count{component="sapor"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri) > 0.5
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: Latency of the component {{ $labels.component }} in namespace {{$labels.kubernetes_namespace}} is high
      description: "Latency of the component {{ $labels.component }} is {{ $value }} seconds for {{ $labels }}"
  - alert: oes-visibility-latency-too-high
    expr: sum(rate(http_server_requests_seconds_sum{component="visibility"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri)/sum(rate(http_server_requests_seconds_count{component="visibility"}[2m])) by (kubernetes_pod_name, method, outcome, status, component, kubernetes_namespace, uri) > 0.5
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: Latency of the component {{ $labels.component }} in namespace {{$labels.kubernetes_namespace}} is high
      description: "Latency of the component {{ $labels.component }} is {{ $value }} seconds for {{ $labels }}"
- name: kube-api-server-is-down
  rules:
  - alert: kube-api-server-down
    expr: up{job="kubernetes-apiservers"} == 0
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: Kube API Server job {{ $labels.job }} is down
      description: "Kubernetes API Server service went down LABELS = {{ $labels }}"
- name: autopilot-component-jvm-errors
  rules:
  - alert: jvm-memory-filling-up-for-oes-audit-client
    expr: (sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_used_bytes{app="oes", area="heap", component="auditclient"}) / sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_max_bytes{app="oes", area="heap", component="auditclient"})) * 100 > 90
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: JVM memory filling up for {{ $labels.component }} for pod {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }})
      description: "JVM memory is filling up for {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }} (> 80%)\n  VALUE = {{ $value }}"
  - alert: jvm-memory-filling-up-for-oes-autopilot
    expr: (sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_used_bytes{app="oes", area="heap", component="autopilot"}) / sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_max_bytes{app="oes", area="heap", component="autopilot"})) * 100 > 90
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: JVM memory filling up for {{ $labels.component }} for pod {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }})
      description: "JVM memory is filling up for {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }} (> 80%)\n  VALUE = {{ $value }}"
  - alert: jvm-memory-filling-up-for-oes-dashboard
    expr: (sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_used_bytes{app="oes", area="heap", component="dashboard"}) / sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_max_bytes{app="oes", area="heap", component="autopilot"})) * 100 > 90
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: JVM memory filling up for {{ $labels.component }} for pod {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }})
      description: "JVM memory is filling up for {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }} (> 80%)\n  VALUE = {{ $value }}"
  - alert: jvm-memory-filling-up-for-oes-platform
    expr: (sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_used_bytes{app="oes", area="heap", component="platform"}) / sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_max_bytes{app="oes", area="heap", component="platform"})) * 100 > 90
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: JVM memory filling up for {{ $labels.component }} for pod {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }})
      description: "JVM memory is filling up for {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }} (> 80%)\n  VALUE = {{ $value }}"
  - alert: jvm-memory-filling-up-for-oes-sapor
    expr: (sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_used_bytes{app="oes", area="heap", component="sapor"}) / sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_max_bytes{app="oes", area="heap", component="sapor"})) * 100 > 90
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: JVM memory filling up for {{ $labels.component }} for pod {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }})
      description: "JVM memory is filling up for {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }} (> 80%)\n  VALUE = {{ $value }}"
  - alert: jvm-memory-filling-up-for-oes-visibility
    expr: (sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_used_bytes{app="oes", area="heap", component="visibility"}) / sum by (instance, kubernetes_pod_name, component, kubernetes_namespace)(jvm_memory_max_bytes{app="oes", area="heap", component="visibility"})) * 100 > 90
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: JVM memory filling up for {{ $labels.component }} for pod {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }})
      description: "JVM memory is filling up for {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }} (> 80%)\n  VALUE = {{ $value }}"
