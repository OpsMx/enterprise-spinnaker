---
# Source: oes/charts/spinnaker/templates/rbac/halyard-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: RELEASE-NAME-spinnaker-halyard
  namespace: oes
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"
---
# Source: oes/charts/openldap/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: RELEASE-NAME-openldap
  labels:
    app: openldap
    chart: openldap-1.2.3
    release: RELEASE-NAME
    heritage: Helm
type: Opaque
data:
  LDAP_ADMIN_PASSWORD: "YWRtaW4="
  LDAP_CONFIG_PASSWORD: "MnN4eGxSdUwxbFBsM2lZdW5HSHR5TE1xdnMydTF0TUM="
---
# Source: oes/charts/spinnaker/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: RELEASE-NAME-minio
  labels:
    app: minio
    chart: minio-1.6.3
    release: RELEASE-NAME
    heritage: Helm
type: Opaque
data:
  accesskey: c3Bpbm5ha2VyYWRtaW4=
  secretkey: c3Bpbm5ha2VyYWRtaW4=
---
# Source: oes/charts/spinnaker/charts/redis/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: RELEASE-NAME-redis
  labels:
    app: redis
    chart: redis-3.8.0
    release: "RELEASE-NAME"
    heritage: "Helm"
type: Opaque
data:
  redis-password: "cGFzc3dvcmQ="
---
# Source: oes/charts/spinnaker/templates/secrets/registry.yaml
apiVersion: v1
kind: Secret
metadata:
  name: RELEASE-NAME-spinnaker-registry
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"
    component: clouddriver
type: Opaque
data:
  dockerhub: ""
---
# Source: oes/templates/imagepull-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: oes-repo
  labels:
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: eyJhdXRocyI6eyJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOnsidXNlcm5hbWUiOiJkb2NrZXJIdWJJZCIsInBhc3N3b3JkIjoiZG9ja2VySHViUHdkIiwiZW1haWwiOiJhYmNAeHl6LmNvbSIsImF1dGgiOiJaRzlqYTJWeVNIVmlTV1E2Wkc5amEyVnlTSFZpVUhkayJ9fX0=
---
# Source: oes/charts/openldap/templates/configmap-env.yaml
#
# A ConfigMap spec for openldap slapd that map directly to env variables in the Pod.
# List of environment variables supported is from the docker image:
# https://github.com/osixia/docker-openldap#beginner-guide
# Note that passwords are defined as secrets
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-openldap-env
  labels:
    app: openldap
    chart: openldap-1.2.3
    release: RELEASE-NAME
    heritage: Helm
data:
  LDAP_BACKEND: hdb
  LDAP_DOMAIN: example.org
  LDAP_ORGANISATION: Example Inc.
  LDAP_REMOVE_CONFIG_AFTER_SETUP: "true"
  LDAP_TLS: "true"
  LDAP_TLS_ENFORCE: "false"
---
# Source: oes/charts/spinnaker/charts/minio/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-minio
  labels:
    app: minio
    chart: minio-1.6.3
    release: RELEASE-NAME
    heritage: Helm
data:
  initialize: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for Minio service to be available
    connectToMinio() {
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/accesskey) ; SECRET=$(cat /config/secretkey) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to Minio server: http://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="mc config host add myminio http://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkBucketExists ($bucket)
    # Check if the bucket exists, by using the exit code of `mc ls`
    checkBucketExists() {
      BUCKET=$1
      CMD=$(/usr/bin/mc ls myminio/$BUCKET > /dev/null 2>&1)
      return $?
    }
    
    # createBucket ($bucket, $policy, $purge)
    # Ensure bucket exists, purging if asked to
    createBucket() {
      BUCKET=$1
      POLICY=$2
      PURGE=$3
    
      # Purge the bucket, if set & exists
      # Since PURGE is user input, check explicitly for `true`
      if [ $PURGE = true ]; then
        if checkBucketExists $BUCKET ; then
          echo "Purging bucket '$BUCKET'."
          set +e ; # don't exit if this fails
          /usr/bin/mc rm -r --force myminio/$BUCKET
          set -e ; # reset `e` as active
        else
          echo "Bucket '$BUCKET' does not exist, skipping purge."
        fi
      fi
    
      # Create the bucket if it does not exist
      if ! checkBucketExists $BUCKET ; then
        echo "Creating bucket '$BUCKET'"
        /usr/bin/mc mb myminio/$BUCKET
      else
        echo "Bucket '$BUCKET' already exists."
      fi
    
      # At this point, the bucket should exist, skip checking for existence
      # Set policy on the bucket
      echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
      /usr/bin/mc policy $POLICY myminio/$BUCKET
    }
    
    # Try connecting to Minio instance
    connectToMinio
    # Create the bucket
    createBucket bucket none false
    
  config.json: |-
    {
      "version": "26",
      "credential": {
        "accessKey": "spinnakeradmin",
        "secretKey": "spinnakeradmin"
      },
      "region": "us-east-1",
      "browser": "on",
      "worm": "off",
      "domain": "",
      "storageclass": {
        "standard": "",
        "rrs": ""
      },
      "cache": {
        "drives": [],
        "expiry": 90,
        "maxuse": 80,
        "exclude": []
      },
      "notify": {
        "amqp": {
          "1": {
            "enable": false,
            "url": "",
            "exchange": "",
            "routingKey": "",
            "exchangeType": "",
            "deliveryMode": 0,
            "mandatory": false,
            "immediate": false,
            "durable": false,
            "internal": false,
            "noWait": false,
            "autoDeleted": false
          }
        },
        "nats": {
          "1": {
            "enable": false,
            "address": "",
            "subject": "",
            "username": "",
            "password": "",
            "token": "",
            "secure": false,
            "pingInterval": 0,
            "streaming": {
              "enable": false,
              "clusterID": "",
              "clientID": "",
              "async": false,
              "maxPubAcksInflight": 0
            }
          }
        },
        "elasticsearch": {
          "1": {
            "enable": false,
            "format": "namespace",
            "url": "",
            "index": ""
          }
        },
        "redis": {
          "1": {
            "enable": false,
            "format": "namespace",
            "address": "",
            "password": "",
            "key": ""
          }
        },
        "postgresql": {
          "1": {
            "enable": false,
            "format": "namespace",
            "connectionString": "",
            "table": "",
            "host": "",
            "port": "",
            "user": "",
            "password": "",
            "database": ""
          }
        },
        "kafka": {
          "1": {
            "enable": false,
            "brokers": null,
            "topic": ""
          }
        },
        "webhook": {
          "1": {
            "enable": false,
            "endpoint": ""
          }
        },
        "mysql": {
          "1": {
            "enable": false,
            "format": "namespace",
            "dsnString": "",
            "table": "",
            "host": "",
            "port": "",
            "user": "",
            "password": "",
            "database": ""
          }
        },
        "mqtt": {
          "1": {
            "enable": false,
            "broker": "",
            "topic": "",
            "qos": 0,
            "clientId": "",
            "username": "",
            "password": "",
            "reconnectInterval": 0,
            "keepAliveInterval": 0
          }
        }
      }
    }
---
# Source: oes/charts/spinnaker/templates/configmap/additional-profile-configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-spinnaker-additional-profile-config-maps
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"
data:
  front50-local.yml: |-
    spinnaker:
      s3:
        versioning: false
  gate-local.yml: |-
    server:
      tomcat:
        httpsServerPort: X-Forwarded-Port
        internalProxies: .*
        protocolHeader: X-Forwarded-Proto
        remoteIpHeader: X-Forwarded-For
---
# Source: oes/charts/spinnaker/templates/configmap/halyard-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-spinnaker-halyard-config
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"
data:
  install.sh: |
    #!/bin/bash

    # Wait for the Hal daemon to be ready
    export DAEMON_ENDPOINT=http://RELEASE-NAME-spinnaker-halyard:8064
    export HAL_COMMAND="hal --daemon-endpoint $DAEMON_ENDPOINT"
    until $HAL_COMMAND --ready; do sleep 10 ; done

    bash -xe /opt/halyard/scripts/config.sh

    $HAL_COMMAND deploy apply
  clean.sh: |
    export HAL_COMMAND='hal --daemon-endpoint http://RELEASE-NAME-spinnaker-halyard:8064'
    $HAL_COMMAND deploy clean -q
  config.sh: |
    # Spinnaker version
    
    $HAL_COMMAND config version edit --version 1.20.5
    

    # Storage
    
    echo spinnakeradmin | $HAL_COMMAND config storage s3 edit \
        --endpoint http://RELEASE-NAME-minio:9000 \
        --access-key-id spinnakeradmin \
        --secret-access-key --bucket spinnaker \
        --path-style-access true
    $HAL_COMMAND config storage edit --type s3
    
    
    
    

    # Docker Registry
    $HAL_COMMAND config provider docker-registry enable

    if $HAL_COMMAND config provider docker-registry account get dockerhub; then
      PROVIDER_COMMAND='edit'
    else
      PROVIDER_COMMAND='add'
    fi

    CREDS=""
    

    $HAL_COMMAND config provider docker-registry account $PROVIDER_COMMAND dockerhub --address index.docker.io \
        ${CREDS}  --repositories library/alpine,library/ubuntu,library/centos,library/nginx

    $HAL_COMMAND config provider kubernetes enable

    if $HAL_COMMAND config provider kubernetes account get default; then
      PROVIDER_COMMAND='edit'
    else
      PROVIDER_COMMAND='add'
    fi

    $HAL_COMMAND config provider kubernetes account $PROVIDER_COMMAND default --docker-registries dockerhub \
                --context default --service-account true \
                 \
                --only-spinnaker-managed true \
                 \
                --omit-namespaces=kube-system,kube-public \
                 \
                 \
                --provider-version v2
    $HAL_COMMAND config deploy edit --account-name default --type distributed \
                           --location oes
    # Use Deck to route to Gate
    $HAL_COMMAND config security api edit --no-validate --override-base-url /gate
    $HAL_COMMAND config features edit --artifacts true
    $HAL_COMMAND config features edit --pipeline-templates true
    $HAL_COMMAND config features edit --managed-pipeline-templates-v2-ui true
---
# Source: oes/charts/spinnaker/templates/configmap/halyard-init-script.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-spinnaker-halyard-init-script
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"
data:
  init.sh: |
    #!/bin/bash

    # Override Halyard daemon's listen address
    cp /opt/halyard/config/* /tmp/config
    printf 'server.address: 0.0.0.0\n' > /tmp/config/halyard-local.yml

    # Use Redis deployed via the dependent Helm chart
    rm -rf /tmp/spinnaker/.hal/default/service-settings
    mkdir -p /tmp/spinnaker/.hal/default/service-settings
    cp /tmp/service-settings/* /tmp/spinnaker/.hal/default/service-settings/

    rm -rf /tmp/spinnaker/.hal/default/profiles
    mkdir -p /tmp/spinnaker/.hal/default/profiles
    cp /tmp/additionalProfileConfigMaps/* /tmp/spinnaker/.hal/default/profiles/

    rm -rf /tmp/spinnaker/.hal/.boms
---
# Source: oes/charts/spinnaker/templates/configmap/service-settings.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-spinnaker-service-settings
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"

data:
  clouddriver.yml: 'artifactId: docker.io/opsmxdev/ubi8-oes-clouddriver:6.9.2'
  deck.yml: |-
    artifactId: docker.io/opsmxdev/ubi8-oes-deck:3.2.0
    env:
      API_HOST: http://spin-gate:8084
  echo.yml: 'artifactId: docker.io/opsmxdev/ubi8-oes-echo:version-2.12.1'
  fial.yml: 'artifactId: docker.io/opsmxdev/ubi8-oes-fiat:1.11.0'
  front50.yml: 'artifactId: docker.io/opsmxdev/ubi8-oes-front50:0.24.0'
  gate.yml: |-
    artifactId: docker.io/devopsmx/ubi8-oes-gate:1.16.1
    healthEndpoint: /health
    kubernetes:
      useExecHealthCheck: false
  igor.yml: 'artifactId: docker.io/opsmxdev/ubi8-oes-igor:1.10.0'
  kayenta.yml: 'artifactId: docker.io/opsmxdev/ubi8-oes-kayenta:0.15.1'
  orca.yml: 'artifactId: docker.io/opsmxdev/ubi8-oes-orca:2.14.2-mj2'
  redis.yml: |-
    overrideBaseUrl: redis://:password@RELEASE-NAME-redis-master:6379
    skipLifeCycleManagement: true
  rosco.yml: 'artifactId: docker.io/opsmxdev/ubi8-oes-rosco:0.19.0'
---
# Source: oes/templates/oes-gate-cm.yaml
apiVersion: v1
data:
  gate-local.yml: |
    server:
      tomcat:
        httpsServerPort: X-Forwarded-Port
        internalProxies: .*
        protocolHeader: X-Forwarded-Proto
        remoteIpHeader: X-Forwarded-For
  gate.yml: |
    spectator:
      applicationName: ${spring.application.name}
      webEndpoint:
        enabled: true
    server:
      ssl:
        enabled: false
      port: '8084'
      address: 0.0.0.0
    security:
      user: {}
    services:
      opsmx:
        baseUrl: http://sapor:8085
        enabled: true
    cors:
      allowed-origins-pattern: ^https?://(?:localhost|OES_UI_LOADBALANCER_IP|opsmx.com)(?::[1-9]\d*)?/?
    ldap:
      enabled: true
      url: ldap://RELEASE-NAME-openldap:389/dc=example,dc=org
      userDnPattern: cn={0}
    google: {}
    integrations:
      gremlin:
        enabled: false
        baseUrl: https://api.gremlin.com/v1
    redis:
      connection: redis://:password@RELEASE-NAME-redis-master:6379

kind: ConfigMap
metadata:
  name: oes-gate-config
  labels:
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
---
# Source: oes/templates/oes-sapor-config-cm.yaml
apiVersion: v1
data:
  audit.yml: |
    services:
      audit:
        applicationRefreshIntervalMs: 50000
  persistence.yml: |
    spring:
      datasource:
        url: jdbc:postgresql://db-opsmx:5432/oesdb
        username: postgres
        password: networks123
        driver-class-name: org.postgresql.Driver
        testWhileIdle: true
        test-on-borrow: true
        hikari:
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1000
          maxLifetime: 1800000
      jpa:
        show-sql: true
        hibernate:
          ddl-auto: update # delete this property after table creation
        properties:
          hibernate:
            dialect: org.hibernate.dialect.PostgreSQLDialect
  
    logging:
      level:
        com:
          zaxxer:
            hikari: DEBUG
  spinnaker.yaml: |
    spinnaker:
      base-url: http://spin-gate:8084/
      external-base-url: http://SPIN_GATE_LOADBALANCER_IP_PORT/
      username: dummyuser
      password: dummypwd
      adminLoginEnabled: false
      adminUsername:
      adminPassword:
      authnEnabled: false

kind: ConfigMap
metadata:
  name: sapor-config
  labels:
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
---
# Source: oes/templates/oes-ui-cm.yaml
apiVersion: v1
data:
  app-config.json: |
    {
        "endPointUrl":"http://OES_GATE_IP:8084/"
    }

kind: ConfigMap
metadata:
  name: oes-ui-config
  labels:
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
---
# Source: oes/charts/spinnaker/charts/minio/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: RELEASE-NAME-minio
  labels:
    app: minio
    chart: minio-1.6.3
    release: RELEASE-NAME
    heritage: Helm
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "10Gi"
---
# Source: oes/charts/spinnaker/templates/rbac/spinnaker-sa.yaml
# In the case of a local cluster Spinnaker needs
# to be able to deploy to all namespaces in the cluster.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: RELEASE-NAME-spinnaker-spinnaker
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- namespace: oes
  kind: ServiceAccount
  # Clouddriver does not currently allow config of its
  # service account.
  name: default
---
# Source: oes/charts/spinnaker/templates/rbac/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: RELEASE-NAME-spinnaker-halyard
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit
subjects:
- namespace: oes
  kind: ServiceAccount
  name: RELEASE-NAME-spinnaker-halyard
---
# Source: oes/templates/rbac/oes-init-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: oes-access
  namespace: oes
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
- namespace: oes
  kind: ServiceAccount
  name: default
---
# Source: oes/charts/openldap/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-openldap
  labels:
    app: openldap
    chart: openldap-1.2.3
    release: RELEASE-NAME
    heritage: Helm
spec:
  ports:
    - name: ldap-port
      protocol: TCP
      port: 389
      targetPort: ldap-port
    - name: ssl-ldap-port
      protocol: TCP
      port: 636
      targetPort: ssl-ldap-port
  selector:
    app: openldap
    release: RELEASE-NAME
  type: ClusterIP
---
# Source: oes/charts/spinnaker/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-minio
  labels:
    app: minio
    chart: minio-1.6.3
    release: RELEASE-NAME
    heritage: Helm
spec:
  type: ClusterIP
  clusterIP: None
  
  ports:
    - name: service
      port: 9000
      targetPort: 9000
      protocol: TCP
  selector:
    app: minio
    release: RELEASE-NAME
---
# Source: oes/charts/spinnaker/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-redis-master
  labels:
    app: redis
    chart: redis-3.8.0
    release: "RELEASE-NAME"
    heritage: "Helm"
  annotations:
spec:
  type: ClusterIP
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: redis
    release: "RELEASE-NAME"
    role: master
---
# Source: oes/charts/spinnaker/templates/services/halyard.yaml
apiVersion: v1
kind: Service
metadata:
  name: RELEASE-NAME-spinnaker-halyard
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"
    component: halyard
spec:
  ports:
  - port: 8064
    name: daemon
  clusterIP: None
  selector:
    app: RELEASE-NAME-spinnaker
    component: halyard
---
# Source: oes/templates/oes-gate-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: oes
    component: oes-gate
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
  name: oes-gate
spec:
  type: LoadBalancer
  ports:
  - name: "oes-gate-service"
    port: 8084
    protocol: TCP
    targetPort: 8084
  selector:
    app: oes
    component: oes-gate
---
# Source: oes/templates/oes-sapor-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: oes
    component: sapor
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
  name: sapor
spec:
  type: ClusterIP
  ports:
  - name: "sapor"
    port: 8085
    targetPort: 8085
  selector:
    app: oes
    component: sapor
---
# Source: oes/templates/oes-ui-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: oes
    component: oes-ui
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
  name: oes-ui
spec:
  type: LoadBalancer
  ports:
  - name: "oes-ui-service"
    port: 80
    targetPort: 80
  selector:
    app: oes
    component: oes-ui
---
# Source: oes/templates/opa-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: opa
  labels:
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
spec:
  selector:
    app: oes
    component: opa
  ports:
  - protocol: TCP
    port: 8181
    targetPort: 8181
  type: ClusterIP
---
# Source: oes/templates/spinnaker-extra/spinsvcs.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: spin
    stack: deck
  name: spin-deck-ui
spec:
  type: LoadBalancer
  ports:
  -  name: http
     port: 9000
     protocol: TCP
  selector:
    cluster: "spin-deck"
---
# Source: oes/charts/openldap/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  RELEASE-NAME-openldap
  labels:
    app: openldap
    chart: openldap-1.2.3
    release: RELEASE-NAME
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: openldap
      release: RELEASE-NAME
  template:
    metadata:
      annotations:
        checksum/configmap-env: 8a43b322aa532590fa95ecb9368b196752e1da07cd680cceb5ffc026d857dc01
      labels:
        app: openldap
        release: RELEASE-NAME
    spec:
      containers:
        - name: openldap
          image: "osixia/openldap:1.2.4"
          imagePullPolicy: IfNotPresent
          ports:
            - name: ldap-port
              containerPort: 389
            - name: ssl-ldap-port
              containerPort: 636
          envFrom:
            - configMapRef:
                name: RELEASE-NAME-openldap-env
            - secretRef:
                name: RELEASE-NAME-openldap
          volumeMounts:
            - name: data
              mountPath: /var/lib/ldap
              subPath: data
            - name: data
              mountPath: /etc/ldap/slapd.d
              subPath: config-data
          env:
          livenessProbe:
            tcpSocket:
              port: ldap-port
            initialDelaySeconds: 20
            periodSeconds: 10
            failureThreshold: 10
          readinessProbe:
            tcpSocket:
              port: ldap-port
            initialDelaySeconds: 20
            periodSeconds: 10
            failureThreshold: 10
          resources:
            {}
      volumes:
        - name: certs
          emptyDir:
            medium: Memory
        - name: data
          emptyDir: {}
---
# Source: oes/charts/spinnaker/charts/minio/templates/deployment.yaml
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: RELEASE-NAME-minio
  labels:
    app: minio
    chart: minio-1.6.3
    release: RELEASE-NAME
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: minio
      release: RELEASE-NAME
  template:
    metadata:
      name: RELEASE-NAME-minio
      labels:
        app: minio
        release: RELEASE-NAME
    spec:
      containers:
        - name: minio
          image: "minio/minio:RELEASE.2018-08-25T01-56-38Z"
          imagePullPolicy: IfNotPresent
          command: [ "/bin/sh", 
          "-ce", 
          "cp /tmp/config.json /root/.minio/ && 
          /usr/bin/docker-entrypoint.sh minio -C /root/.minio/ server /export" ]
          volumeMounts:
            - name: export
              mountPath: /export
            - name: minio-server-config
              mountPath: "/tmp/config.json"
              subPath: config.json
            - name: minio-config-dir
              mountPath: /root/.minio/
          ports:
            - name: service
              containerPort: 9000
          env:
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: RELEASE-NAME-minio
                  key: accesskey
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: RELEASE-NAME-minio
                  key: secretkey
          livenessProbe:
            tcpSocket:
              port: 9000
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
      volumes:
        - name: export
          persistentVolumeClaim:
            claimName: RELEASE-NAME-minio
        - name: minio-server-config
          configMap:
            name: RELEASE-NAME-minio
        - name: minio-user
          secret:
            secretName: RELEASE-NAME-minio
        - name: minio-config-dir
          emptyDir: {}
---
# Source: oes/templates/oes-gate-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: oes
    component: oes-gate
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
  name: oes-gate
spec:
  replicas: 1
  selector:
    matchLabels:
      app: oes
      component: oes-gate
  template:
    metadata:
      annotations:
        configmap/checksum: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      labels:
        app: oes
        component: oes-gate
        heritage: "Helm"
        release: "RELEASE-NAME"
        chart: "oes-2.0.0"
    spec:
      initContainers:
      - env:
        - name: EXTERNAL_IP_CHECK_DELAY
          value: "180"
        - name: SPINNAKER_SETUP_DELAY
          value: "180"
        image: opsmx11/oes-init:v2
        imagePullPolicy: Always
        name: oes-gate-init
        command: ["/home/config_endpoint.sh"]
        args: ["oes-gate"]
        volumeMounts:
        - mountPath: /config
          name: config-temp
        - mountPath: /opt/spinnaker/config
          name: gate-volume
      containers:
      - image: opsmx11/oes-gate:v0.202007101453
        name: oes-gate
        ports:
        - containerPort: 8084
          protocol: TCP
        volumeMounts:
        - name: gate-volume
          mountPath: /opt/spinnaker/config
        readinessProbe:
          tcpSocket:
            port: 8084
          initialDelaySeconds: 120
          periodSeconds: 30
        livenessProbe:
          httpGet:
            path: /health
            port: 8084
          initialDelaySeconds: 120
          periodSeconds: 60
      imagePullSecrets:
      - name: oes-repo
      volumes:
      - configMap:
          name: oes-gate-config
        name: config-temp
      - emptyDir: {}
        name: gate-volume
---
# Source: oes/templates/oes-sapor-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: oes
    component: sapor
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
  name: sapor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: oes
      component: sapor
  template:
    metadata:
      annotations:
        configmap/checksum: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      labels:
        app: oes
        component: sapor
        heritage: "Helm"
        release: "RELEASE-NAME"
        chart: "oes-2.0.0"
    spec:
      initContainers:
      - env:
        - name: EXTERNAL_IP_CHECK_DELAY
          value: "180"
        - name: SPINNAKER_SETUP_DELAY
          value: "180"
        image: opsmx11/oes-init:v2
        imagePullPolicy: Always
        name: sapor-init
        command: ["/home/config_endpoint.sh"]
        args: ["sapor"]
        volumeMounts:
        - mountPath: /config
          name: config-temp
        - mountPath: /opt/opsmx/
          name: api-volume
      containers:
      - image: opsmx11/sapor:v0.202007101007
        name: sapor
        ports:
        - containerPort: 8085
          protocol: TCP
        volumeMounts:
        - name: api-volume
          mountPath: /opt/opsmx/
        readinessProbe:
          tcpSocket:
            port: 8085
          initialDelaySeconds: 120
          periodSeconds: 30
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8085
          initialDelaySeconds: 180
          periodSeconds: 60
      imagePullSecrets:
      - name: regcred
      volumes:
      - configMap:
          name: sapor-config
        name: config-temp
      - emptyDir: {}
        name: api-volume
---
# Source: oes/templates/oes-ui-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: oes
    component: oes-ui
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
  name: oes-ui
spec:
  replicas: 1
  selector:
    matchLabels:
      app: oes
      component: oes-ui
  template:
    metadata:
      annotations:
        configmap/checksum: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
      labels:
        app: oes
        component: oes-ui
        heritage: "Helm"
        release: "RELEASE-NAME"
        chart: "oes-2.0.0"
    spec:
      initContainers:
      - env:
        - name: EXTERNAL_IP_CHECK_DELAY
          value: "180"
        - name: SPINNAKER_SETUP_DELAY
          value: "180"
        image: opsmx11/oes-init:v2
        imagePullPolicy: Always
        name: oes-ui-init
        command: ["/home/config_endpoint.sh"]
        args: ["oes-ui"]
        volumeMounts:
        - mountPath: /config
          name: config-temp
        - mountPath: /var/www/html/assets/config
          name: config-dir
      containers:
      - image: opsmx11/oes-ui:v0.202007011835
        name: oes-ui
        ports:
        - containerPort: 80
          protocol: TCP
        volumeMounts:
        - name: config-dir
          mountPath: /var/www/html/assets/config
        readinessProbe:
          tcpSocket:
            port: 80
          initialDelaySeconds: 15
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 15
          periodSeconds: 5
      imagePullSecrets:
      - name: oes-repo
      volumes:
      - configMap:
          defaultMode: 420
          name: oes-ui-config
        name: config-temp
      - emptyDir: {}
        name: config-dir
---
# Source: oes/templates/opa-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: oes
    component: opa
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "oes-2.0.0"
  name: opa
spec:
  replicas: 1
  selector:
    matchLabels:
      app: oes
      component: opa
  template:
    metadata:
      labels:
        app: oes
        component: opa
        heritage: "Helm"
        release: "RELEASE-NAME"
        chart: "oes-2.0.0"
      name: opa
    spec:
      containers:
        - name: opa
          image: openpolicyagent/opa:latest
          args:
            - "run"
            - "--server"
---
# Source: oes/charts/spinnaker/charts/redis/templates/redis-master-statefulset.yaml
apiVersion: apps/v1beta2
kind: StatefulSet
metadata:
  name: RELEASE-NAME-redis-master
  labels:
    app: redis
    chart: redis-3.8.0
    release: "RELEASE-NAME"
    heritage: "Helm"
spec:
  selector:
    matchLabels:
      release: "RELEASE-NAME"
      chart: redis-3.8.0
      role: master
      app: redis
  serviceName: "redis-master"
  template:
    metadata:
      labels:
        release: "RELEASE-NAME"
        chart: redis-3.8.0
        role: master
        app: redis
    spec:
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
      serviceAccountName: "default"
      containers:
      - name: RELEASE-NAME-redis
        image: "docker.io/bitnami/redis:4.0.11-debian-9"
        imagePullPolicy: "Always"
        env:
        - name: REDIS_REPLICATION_MODE
          value: master
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: RELEASE-NAME-redis
              key: redis-password
        - name: REDIS_DISABLE_COMMANDS
          value: FLUSHDB,FLUSHALL
        ports:
        - name: redis
          containerPort: 6379
        livenessProbe:
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - redis-cli
            - ping
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - redis-cli
            - ping
        resources:
          null
        volumeMounts:
        - name: redis-data
          mountPath: /bitnami/redis/data
          subPath: 
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app: "redis"
          chart: redis-3.8.0
          component: "master"
          release: "RELEASE-NAME"
          heritage: "Helm"
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
  updateStrategy:
    type: OnDelete
---
# Source: oes/charts/spinnaker/templates/statefulsets/halyard.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: RELEASE-NAME-spinnaker-halyard
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"
spec:
  serviceName: RELEASE-NAME-spinnaker-halyard
  replicas: 1
  selector:
    matchLabels:
      app: "RELEASE-NAME-spinnaker"
      release: "RELEASE-NAME"
      component: halyard
  template:
    metadata:
      labels:
        app: "RELEASE-NAME-spinnaker"
        heritage: "Helm"
        release: "RELEASE-NAME"
        chart: "spinnaker-1.22.0"
        component: halyard
    spec:
      serviceAccountName: RELEASE-NAME-spinnaker-halyard
      securityContext:
        runAsUser: 1000
        fsGroup: 1000
      initContainers:
      - name: "create-halyard-local"
        image: docker.io/devopsmx/ubi8-oes-operator-halyard:1.20.5
        command:
        - bash
        - /tmp/initscript/init.sh
        volumeMounts:
        - name: halyard-config
          mountPath: /tmp/config
        - name: service-settings
          mountPath: /tmp/service-settings
        - name: halyard-home
          mountPath: /tmp/spinnaker
        - name: additional-profile-config-maps
          mountPath: /tmp/additionalProfileConfigMaps
        - name: halyard-initscript
          mountPath: /tmp/initscript
      volumes:
      - name: reg-secrets
        secret:
          secretName: RELEASE-NAME-spinnaker-registry
      - name: additional-profile-config-maps
        configMap:
          name: RELEASE-NAME-spinnaker-additional-profile-config-maps
      - name: halyard-config
        emptyDir: {}
      - name: service-settings
        configMap:
          name: RELEASE-NAME-spinnaker-service-settings
      - name: halyard-initscript
        configMap:
          name: RELEASE-NAME-spinnaker-halyard-init-script
      containers:
      - name: halyard
        image: docker.io/devopsmx/ubi8-oes-operator-halyard:1.20.5
        ports:
        - containerPort: 8064
          name: daemon
        volumeMounts:
        - name: halyard-home
          mountPath: /home/spinnaker
        - name: halyard-config
          mountPath: /opt/halyard/config
        - name: reg-secrets
          mountPath: /opt/registry/passwords
  volumeClaimTemplates:
  - metadata:
      name: halyard-home
      labels:
        app: "RELEASE-NAME-spinnaker"
        heritage: "Helm"
        release: "RELEASE-NAME"
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
---
# Source: oes/charts/openldap/templates/configmap-customldif.yaml
#
# A ConfigMap spec for openldap slapd that map directly to files under
# /container/service/slapd/assets/config/bootstrap/ldif/custom
#
---
# Source: oes/charts/spinnaker/templates/hooks/cleanup.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "RELEASE-NAME-spinnaker-cleanup-using-hal"
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"
    component: halyard
  annotations:
    "helm.sh/hook": "pre-delete"
    "helm.sh/hook-delete-policy": "before-hook-creation"
spec:
  template:
    metadata:
      labels:
        app: "RELEASE-NAME-spinnaker"
        heritage: "Helm"
        release: "RELEASE-NAME"
        chart: "spinnaker-1.22.0"
        component: halyard
    spec:
      restartPolicy: OnFailure
      volumes:
      - name: halyard-config
        configMap:
          name: RELEASE-NAME-spinnaker-halyard-config
      containers:
      - name: halyard-install
        image: docker.io/devopsmx/ubi8-oes-operator-halyard:1.20.5
        volumeMounts:
        - name: halyard-config
          mountPath: /opt/halyard/scripts
        command:
        - bash
        - -xe
        - "/opt/halyard/scripts/clean.sh"
---
# Source: oes/charts/spinnaker/templates/hooks/install-using-hal.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "RELEASE-NAME-install-using-hal"
  labels:
    app: "RELEASE-NAME-spinnaker"
    heritage: "Helm"
    release: "RELEASE-NAME"
    chart: "spinnaker-1.22.0"
  annotations:
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-delete-policy": "before-hook-creation"
    "helm.sh/hook-weight": "0"
spec:
  template:
    metadata:
      labels:
        app: "RELEASE-NAME-spinnaker"
        heritage: "Helm"
        release: "RELEASE-NAME"
        chart: "spinnaker-1.22.0"
    spec:
      restartPolicy: OnFailure
      volumes:
      - name: halyard-config
        configMap:
          name: RELEASE-NAME-spinnaker-halyard-config
      containers:
      - name: halyard-install
        image: docker.io/devopsmx/ubi8-oes-operator-halyard:1.20.5
        volumeMounts:
        - name: halyard-config
          mountPath: /opt/halyard/scripts
        command:
        - bash
        - -xe
        - "/opt/halyard/scripts/install.sh"
